{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372a71c6",
   "metadata": {},
   "source": [
    "# Beginner-Friendly Machine Learning Tutorial: Titanic Dataset\n",
    "\n",
    "Welcome! This notebook will guide you step-by-step through a hands-on machine learning workflow using the Titanic dataset. You will learn how to:\n",
    "\n",
    "- Load and explore data\n",
    "- Preprocess and clean data\n",
    "- Build and evaluate a machine learning model\n",
    "- Visualize results\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16bd88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to sys.path so we can import custom modules from anywhere\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911b92c",
   "metadata": {},
   "source": [
    "## 1. Setup: Import Libraries and Configure Environment\n",
    "\n",
    "Let's import the necessary Python libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e30ee8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from src.utility import *\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690524a0",
   "metadata": {},
   "source": [
    "### About the Custom Machine Learning Classes\n",
    "\n",
    "This notebook uses several custom classes from `utility.py` to simplify and enhance the machine learning workflow:\n",
    "\n",
    "- **MetaCleanPipeline**: Automates data cleaning, feature engineering, and target encoding.\n",
    "- **FeatureEngineeringSelector**: Flexible feature selection and engineering.\n",
    "- **DataCleaner**: Handles missing values, scaling, encoding, and duplicate removal.\n",
    "- **GraphAnalyzerEngine**: Analyzes feature importances and relationships.\n",
    "- **VisualizerFactory**: Provides advanced visualizations.\n",
    "\n",
    "These tools make the machine learning process more accessible and interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96803657",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Titanic Dataset\n",
    "\n",
    "The Titanic dataset contains information about passengers and whether they survived. Let's load the data and take a first look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "947d2c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d7364",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing: Feature Selection and Cleaning\n",
    "\n",
    "To prepare our data for machine learning, we need to:\n",
    "- Separate features (inputs) from the target (output)\n",
    "- Clean and preprocess the data (handle missing values, encode categories, etc.)\n",
    "\n",
    "**Note:** The 'sex', 'embarked', and 'class' features are categorical and may require special encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0d08856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean columns converted to string: []\n"
     ]
    }
   ],
   "source": [
    "# Drop rows with missing target (survived)\n",
    "titanic = titanic.dropna(subset=['survived'])\n",
    "# Separate features (X) and target (y)\n",
    "X = titanic.drop(columns=['survived'])\n",
    "y = titanic['survived']\n",
    "\n",
    "# Convert all string columns to categorical\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    X[col] = X[col].astype('category')\n",
    "\n",
    "# Convert boolean columns to string/object dtype for compatibility with SimpleImputer\n",
    "for col in X.select_dtypes(include=['bool']).columns:\n",
    "    X[col] = X[col].astype(str)\n",
    "print('Boolean columns converted to string:', X.select_dtypes(include=['bool']).columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550310cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   who_man  adult_male_True  alive_no  alive_yes\n",
      "0      1.0              1.0       1.0        0.0\n",
      "1      0.0              0.0       0.0        1.0\n",
      "2      0.0              0.0       0.0        1.0\n",
      "3      0.0              0.0       0.0        1.0\n",
      "4      1.0              1.0       1.0        0.0\n",
      "Final features: ['who_man', 'adult_male_True', 'alive_no', 'alive_yes']\n",
      "y_original unique: [0 1]\n",
      "y_enc unique: [0 1]\n",
      "shape X_cleaned: (784, 14)\n",
      "shape y_enc: (784,)\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Create and configure the data cleaning and feature engineering pipeline\n",
    "meta_pipe = MetaCleanPipeline(\n",
    "    drop_duplicates=True,\n",
    "    feature_engineering_strategies=[{\n",
    "        'name': 'model_importance',\n",
    "        'model_cls': RandomForestClassifier,\n",
    "        'threshold': 'mean'\n",
    "    }],\n",
    "    numeric_strategy='mean',\n",
    "    categorical_strategy='most_frequent',\n",
    "    target_encoder_method='label',\n",
    "    auto_encode_features=False,\n",
    "    # You can add more cleaner_kwargs if needed\n",
    ")\n",
    "\n",
    "# Fit the pipeline and transform the data\n",
    "X_clean = meta_pipe.fit_transform(X, y)\n",
    "print(X_clean.head())\n",
    "print('Final features:', meta_pipe.get_selected_features())\n",
    "\n",
    "# Retrieve cleaned features and encoded target for training\n",
    "X_cleaned, y_enc = meta_pipe.get_cleaned_dataset()\n",
    "y_original = meta_pipe.inverse_transform_target(y_enc)\n",
    "print('y_original unique:', np.unique(y_original))\n",
    "print('y_enc unique:', np.unique(y_enc))\n",
    "print('shape X_cleaned:', X_cleaned.shape)\n",
    "print('shape y_enc:', y_enc.shape)\n",
    "y_full_enc = meta_pipe.transform_target(y_original)\n",
    "\n",
    "# If you want to decode y_enc back to original labels:\n",
    "print(meta_pipe.inverse_transform_target(np.unique(y_enc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58a632a",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation\n",
    "\n",
    "Now, let's train a machine learning model to predict survival. We'll:\n",
    "- Split the data into training and test sets\n",
    "- Train a Random Forest classifier\n",
    "- Evaluate the model's performance using a classification report and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3a790b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['who_man', 'adult_male_True', 'alive_no', 'alive_yes'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[0;32m      2\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))\n\u001b[0;32m      3\u001b[0m ])\n\u001b[1;32m----> 5\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\u001b[43mX_cleaned\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmeta_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_selected_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m,\n\u001b[0;32m      6\u001b[0m                                                     y_original, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      7\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      8\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Mehdi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Mehdi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mehdi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['who_man', 'adult_male_True', 'alive_no', 'alive_yes'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([\n",
    "    ('model', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_cleaned[meta_pipe.get_selected_features()],\n",
    "                                                    y_original, random_state=42)\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print('Classification Report:\\n', classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_original), yticklabels=np.unique(y_original), ax=ax)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02178478",
   "metadata": {},
   "source": [
    "## 5. Feature Importance and Visualization\n",
    "\n",
    "Let's use advanced visualizations to explore feature importances and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical features to numeric codes for visualization\n",
    "for col in X_cleaned.select_dtypes(include=['category']).columns:\n",
    "    X_cleaned[col] = X_cleaned[col].cat.codes\n",
    "\n",
    "# Fill missing values in X_cleaned\n",
    "X_cleaned = X_cleaned.fillna(X_cleaned.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2127ad83",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = GraphAnalyzerEngine()\n",
    "engine.analyze(X_cleaned, pd.DataFrame(y_enc))\n",
    "store = engine.get_store()\n",
    "label_map = dict(enumerate(meta_pipe.get_target_encoder().classes_))\n",
    "relabel_targets_in_store(store, label_map)\n",
    "fig_sankey = VisualizerFactory.make_sankey(store, show_feature_feature_links=False)\n",
    "fig_sankey.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0377ec8",
   "metadata": {},
   "source": [
    "### Bar Plot of Feature Importances\n",
    "\n",
    "The bar plot below shows the aggregated importance of each feature. The dashed line represents the threshold for random (noise) importance. Features above this line are considered informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791091eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_bar = VisualizerFactory.make_bar(store, show_threshold=True, show_noise=True)\n",
    "fig_bar.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0faa03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot t-SNE map for feature visualization\n",
    "fig_tsne = VisualizerFactory.make_tsne(X_cleaned, y_enc,\n",
    "                                    perplexity=5,\n",
    "                                    random_state=10)\n",
    "fig_tsne.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fdb535",
   "metadata": {},
   "source": [
    "## 6. Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You have completed a full machine learning workflow on the Titanic dataset:\n",
    "- Data loading and exploration\n",
    "- Preprocessing and cleaning\n",
    "- Model training and evaluation\n",
    "- Feature importance analysis and visualization\n",
    "\n",
    "Feel free to experiment with different models, parameters, or datasets!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
